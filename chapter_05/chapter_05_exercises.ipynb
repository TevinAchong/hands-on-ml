{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the fundamental idea behind support vector machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer:**\n",
    "- A support vector machine is a machine learning algorithm that can be used for classification, regression or even anomaly detection. The main idea is that it finds a \"street\" that is used to separate instances (classification) or predict the value of instances (regression). This is done by finding the line that is as far away from either side of the street as possible. The sides of the street are created by what are known as support vectors. By adjusting various hyperparameters we can adjust how tolerant our model is to allowing instances on the street. The ultimate objective: keep as many instances off the street as possible, but still give the model room to generalize well.\n",
    "\n",
    "\n",
    "**Aurelien's Answer:**\n",
    "- The fundamental idea behind Support Vector Machines is to fit the widest possible \"street\" between the classes. In other words, the goal is to have the largest possible margin between the decision boundary that separates the two classes and the training instances. When performing soft-margin classification, the SVM searches for a compromise between perfectly separating the two classes and having the widest possible street (i.e., a few instances may end up on the street). Another key idea is to use kernels when training on nonlinear datasets. SVMs can also be tweaked to perform linear and nonlinear regression, as well as novelty detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer:**\n",
    "- Support vectors are the instances that make up the sides of the \"streets\" in support vector machines. The decision boundary is made by finding the balance between separating two classes; this boundary is at the center of two other boundaries. The vectors that make these boundaries, are the support vectors.\n",
    "- **!!! LOOKS LIKE I HAD THIS ONE ALL WRONG !!!**\n",
    "\n",
    "**Aurelien's Answer:**\n",
    "- After training an SVM, a *support vector* is any instance located on the \"street\", including its border. The decision boundary is entirely determined by the support vectors. Any instance that is *not* a support vector (i.e., is off the street) hs no influence whatsoever; you could remove them, add more instances, or move them around, and as long as they stay off the street they won't affect the decision boundary. Computing the predictions with a kernelized SVM only involves the support vectors, not the whole training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Why is it important to scale the inputs when using SVMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer:**\n",
    "- I am not sure. Is it because the algorithm is very sensitive to outliers?\n",
    "\n",
    "**Aurelien's Answer:**\n",
    "- SVMs try to fit the largest possible \"street\" between the classes, so if the training set is not scaled, the SVM will tend to neglect small features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer:**\n",
    "- An SVM classifier does not directly output a confidence score. Instead, the decision function results in either a negative or a positive number. The side of zero that the decision function outputs on determines the class it is categorized into. The further away from zero, the more confident the model is.  In terms of probability, if you use the `SVC` class, and set `probability=True`, then it will output and estimated probability that an instance is in each class.\n",
    "\n",
    "**Aurelien's Answer:**\n",
    "- You can use the `decision_function()` method to get confidence scores. The scores represent the distance between the instance and the decision boundary. However, they cannot be directly converted into an estimation of the class probability. If you set `probability=True` when creating an `SVC`, then at the end of training it will use 5-fold cross validation to generate out-of-sample scores for the training samples, and it will train a `LogisticRegression` model to map these scores to estimated probabilities. The `predict_proba()` and `predict_log_proba()` methods will then be available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How can you choose between `LinearSVC`, `SVC`, and `SGDClassifier`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer:**\n",
    "- If the training set is very large (< 10,000), you should use the `SGDClassifier` or `LinearSVC` as they have the same time complexity, and are both much faster than `SVC(kernel=linear)`. The larger the dataset, the more sense it makes to use `SGDClassifier` as it can handle datasets too big to fit in RAM. \n",
    "- If the training set is not too large, and it has a lot of features (and very few non-zero features) you should use `SVC`.\n",
    "\n",
    "**Aurelien's Answer:**\n",
    "- All three classes can be used for large-margin linear classification. The `SVC` class also supports the kernel trick, which makes it capable of handling nonlinear tasks. However, this comes at a cost: the `SVC` class does not scale well to datasets with many instances. It does scale well to a large number of features, though. The `LinearSVC` class implements an optimized algorithm for linear SVMs, while `SGDClassifier` uses Stochastic Gradient Descent. Depending on the dataset `LinearSVC` may be a bit faster than `SGDClassifier`, but not always, and `SGDClassifier` is more flexible, plus it supports incremental learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Say you've trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease $ \\gamma $ (`gamma`). What about `C`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer:**\n",
    "\n",
    "**Aurelien's Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What does it mean for a model to be $\\epsilon$*-insensitive*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer:**\n",
    "\n",
    "**Aurelien's Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. What is the point of using the kernel trick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer:**\n",
    "\n",
    "**Aurelien's Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Train a `LinearSVC` on a linearly separable dataset. Then train an `SVC` and a `SGDClassifier` on the same dataset. See if you can get them to produce roughly the same model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Train an SVM classifier on the wine dataset, which you can load using `sklearn.datasets.load_wine()`. This dataset contains the chemical analyses of 178 wine samples produced by 3 different cultivators: the goal is to train a classification model capable of predicting the cultivator based on the wine's chemical analysis. Since SVM classifiers are binary classifiers, you will need to use one-versus-all to classify all three classes. What accuracy can you reach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Train and fine-tune an SVM regressor on the California housing dataset. You can use the original dataset rather than the tweaked version we used in Chapter 2, which you can load using `sklearn.datasets.fetch_california_housing()`. The targets represent hundreds of thousands of dollars. Since there are over 20,000 instances, SVMs can be slow, so for hyperparameter tuning you should use far fewer instances (e.g. 2000) to test many more hyperparameter combinations. What is your best model's RMSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
