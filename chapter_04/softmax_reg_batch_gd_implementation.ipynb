{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **12. Implement batch gradient descent with early stopping for softmax regression without using Scikit-Learn, only NumPy. Use it on a classification task such as the iris dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris.data.values\n",
    "X.shape # 150 instances with 4 features each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y has 150 instances\n"
     ]
    }
   ],
   "source": [
    "y = iris.target_names[iris.target]\n",
    "print(f\"y has {y.shape[0]} instances\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2864604 ,  2.43545215, -0.78172474,  0.8268637 ,  0.13746866,\n",
       "        1.97585545,  1.51625875, -1.47111979, -0.55192639,  0.13746866,\n",
       "       -0.09232969,  1.2864604 , -0.32212804, -0.09232969, -0.09232969,\n",
       "        1.05666205, -0.55192639, -0.78172474,  0.8268637 ,  1.05666205,\n",
       "        1.7460571 , -2.39031318, -0.78172474,  0.8268637 , -0.09232969,\n",
       "        0.59706535, -0.78172474,  0.8268637 , -0.09232969,  1.7460571 ,\n",
       "        0.367267  , -0.32212804, -1.24132144,  2.66525049, -0.09232969,\n",
       "       -0.78172474, -1.93071649, -0.09232969,  0.8268637 , -1.70091814,\n",
       "        1.97585545, -1.70091814, -0.78172474,  0.59706535,  0.367267  ,\n",
       "       -1.47111979, -0.09232969,  0.13746866, -0.55192639,  0.8268637 ,\n",
       "        1.05666205,  0.367267  , -1.24132144,  0.367267  , -0.78172474,\n",
       "       -1.01152309, -0.09232969, -1.24132144, -0.32212804, -0.09232969,\n",
       "       -1.93071649,  0.367267  ,  0.13746866,  1.05666205, -0.09232969,\n",
       "       -1.24132144,  0.8268637 ,  0.8268637 , -0.09232969,  0.59706535,\n",
       "        0.367267  , -0.32212804, -0.55192639, -0.55192639,  0.367267  ,\n",
       "        0.367267  ,  1.7460571 ,  0.13746866, -0.09232969, -0.09232969,\n",
       "       -1.01152309, -0.78172474, -0.09232969, -1.70091814, -0.32212804,\n",
       "       -1.01152309,  1.51625875, -0.09232969, -0.32212804,  0.59706535,\n",
       "        1.51625875, -1.47111979,  0.59706535, -0.09232969,  1.2864604 ,\n",
       "        0.13746866, -0.09232969,  0.13746866, -0.55192639, -0.32212804,\n",
       "       -0.55192639, -0.09232969, -1.70091814, -0.32212804, -0.55192639,\n",
       "       -0.09232969,  0.8268637 , -0.55192639, -1.24132144,  2.2056538 ,\n",
       "       -1.01152309, -0.09232969])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train[:, 1] - X_train[:, 1].mean()) / X_train[:, 1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the training data\n",
    "def standard_scale(np_array, col_idxs):\n",
    "    standardized_columns = []\n",
    "    for col_idx in col_idxs:\n",
    "        standardized_columns.append(\n",
    "            (np_array[:, col_idx] - np_array[:, col_idx].mean()) / np_array[:, col_idx].std()\n",
    "        )\n",
    "    return np.column_stack(standardized_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = standard_scale(X_train, [0, 1, 2, 3])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in train set: ['setosa' 'versicolor' 'virginica']\n",
      "Unique values in train set: ['setosa' 'versicolor' 'virginica']\n",
      "Index: 0\n",
      "Actual category: setosa\n",
      "Numeric category: 0\n",
      "**************************************************\n",
      "Index: 1\n",
      "Actual category: setosa\n",
      "Numeric category: 0\n",
      "**************************************************\n",
      "Index: 2\n",
      "Actual category: virginica\n",
      "Numeric category: 2\n",
      "**************************************************\n",
      "Index: 3\n",
      "Actual category: versicolor\n",
      "Numeric category: 1\n",
      "**************************************************\n",
      "Index: 4\n",
      "Actual category: versicolor\n",
      "Numeric category: 1\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# Checking the unique values (categories)\n",
    "print(f'Unique values in train set: {np.unique(y_train)}')\n",
    "print(f'Unique values in train set: {np.unique(y_test)}')\n",
    "\n",
    "# Replacing each value with a numeric value so that we can calculate the gradients\n",
    "y_train_num = []\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == 'setosa':\n",
    "        y_train_num.append(0)\n",
    "    elif y_train[i] == 'versicolor':\n",
    "        y_train_num.append(1)\n",
    "    else:\n",
    "        y_train_num.append(2)\n",
    "\n",
    "# Just testing a few\n",
    "for i in range(0, 5):\n",
    "    print(f'Index: {i}')\n",
    "    print(f'Actual category: {y_train[i]}')\n",
    "    print(f'Numeric category: {y_train_num[i]}')\n",
    "    print('*'*50)\n",
    "\n",
    "y_train_num = np.array(y_train_num).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the same operation for y_test\n",
    "# Replacing each value with a numeric value so that we can calculate the gradients\n",
    "y_test_num = []\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 'setosa':\n",
    "        y_test_num.append(0)\n",
    "    elif y_test[i] == 'versicolor':\n",
    "        y_test_num.append(1)\n",
    "    else:\n",
    "        y_test_num.append(2)\n",
    "\n",
    "y_test_num = np.array(y_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (112, 4)\n",
      "y_train shape: (112, 1)\n",
      "**************************************************\n",
      "X_test shape: (38, 4)\n",
      "y_test shape: (38,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train_num.shape}\")\n",
    "print('*'*50)\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test_num.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since we have 3 classes, and 4 features, our weight matrix will have shape 3 x 4.\n",
      "(3, 4)\n",
      "[-1.01827123  1.2864604  -1.39338902 -1.3621769 ]\n"
     ]
    }
   ],
   "source": [
    "# Each class should have a weight for each feature\n",
    "# Since we have 3 classes, and 4 features\n",
    "# We expect a 3 x 4 matrix (each class' weights for each feature is one row)\n",
    "\n",
    "# So let us randomly initalize the weights\n",
    "num_classes = len(np.unique(y_train_num))\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "print(f\"Since we have {num_classes} classes, and {num_features} features, \\\n",
    "our weight matrix will have shape {num_classes} x {num_features}.\")\n",
    "\n",
    "weight_matrix = np.random.randn(num_classes, num_features)\n",
    "print(weight_matrix.shape)\n",
    "\n",
    "# Logit (raw score) of the first instance being in class 0 with random weights\n",
    "weight_matrix[0, :].T @ X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 0\n",
      "Weights for class 0: [0.74805295 0.20302355 0.21513962 0.46284109]\n",
      "Logit (raw score) for class 0: -1.4307836849935751\n",
      "--------------------\n",
      "Weights for class 1: [ 0.56387081 -1.46685203  0.72642599 -2.48901811]\n",
      "Logit (raw score) for class 1: -0.08293151217984662\n",
      "--------------------\n",
      "Weights for class 2: [-0.88442476  0.3672323  -0.04309255 -0.72483252]\n",
      "Logit (raw score) for class 2: 2.4204088976171265\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "Probability of class 0: 2.0%\n",
      "--------------------\n",
      "Probability of class 1: 7.000000000000001%\n",
      "--------------------\n",
      "Probability of class 2: 91.0%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# About to run batch gradient descent\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.02\n",
    "num_instances = X_train.shape[0]\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    print(f\"Current epoch: {epoch}\")\n",
    "    \n",
    "    # Step 1: Calculate the raw score of each instance being in each class\n",
    "    raw_scores_vector = []\n",
    "    for idx in range(len(weight_matrix)):\n",
    "        class_weights_vector = weight_matrix[idx]\n",
    "        print(f\"Weights for class {idx}: {class_weights_vector}\")\n",
    "        class_raw_score = class_weights_vector.T @ X_train[0]\n",
    "        print(f\"Logit (raw score) for class {idx}: {class_raw_score}\")\n",
    "        print('-'*20)\n",
    "\n",
    "        raw_scores_vector.append(class_raw_score)\n",
    "    raw_scores_vector = np.array(raw_scores_vector)\n",
    "    print('\\n\\n')\n",
    "\n",
    "    # Step 2: Calculating a probability score for each instance\n",
    "    sum_raw_scores_exponents = np.exp(raw_scores_vector).sum()\n",
    "    for idx in range(len(raw_scores_vector)):\n",
    "        prob = np.exp(raw_scores_vector[idx]) / sum_raw_scores_exponents\n",
    "        print(f\"Probability of class {idx}: {round(prob, 2) * 100}%\")\n",
    "        print('-'*20)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.19287485057736"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(np.array([1, 2, 3])).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights: [[ 0.49671415]\n",
      " [-0.1382643 ]\n",
      " [ 0.64768854]\n",
      " [ 1.52302986]]\n",
      "\n",
      "Final theta: [[-0.02153836]\n",
      " [-0.05733346]\n",
      " [ 0.24250165]\n",
      " [ 0.53404859]]\n"
     ]
    }
   ],
   "source": [
    "# Implementing batch gradient descent\n",
    "learning_rate = 0.02\n",
    "num_epochs = 1000\n",
    "num_instances = X_train.shape[0]\n",
    "\n",
    "np.random.seed(42)\n",
    "# Randomly initialize weights for each feature\n",
    "theta = np.random.randn(X_train.shape[1], 1)\n",
    "print(f\"Initial Weights: {theta}\\n\")\n",
    "\n",
    "# \n",
    "for epoch in range(num_epochs):\n",
    "    gradients = 2 / num_instances * X_train.T @ (X_train @ theta - y_train_num)\n",
    "    theta = theta - learning_rate * gradients\n",
    "\n",
    "# Final theta\n",
    "print(f\"Final theta: {theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
