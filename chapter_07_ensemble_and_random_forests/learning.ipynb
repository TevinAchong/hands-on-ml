{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "- If you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. \n",
    "- A group of predictors is called an ***ensemble***; thus, this technique is called ***ensemble learning***, and an ensemble algorithm is called an ***ensemble method***.\n",
    "- For example:\n",
    "    - You can train a group of decision tree classifiers, each on a different random subset of the training set.\n",
    "    - You can then obtain the predictions of all the individual trees, and the class that gets the most votes is the ensemble's prediction.\n",
    "    - Such an ensemble of decision trees is called a ***random forest***, and despite its simplicity, this is one of the most powerful machine learning algorithms available today.\n",
    "- You will often use ensemble methods near the end of a project, once you have already built a few good predictors, to combine them into an even better predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifiers\n",
    "- Suppose you have trained a few classifiers, each one achieving about 80% accuracy.\n",
    "- *You may have a logistic regression classifier, an SVM classifier, a random forest classifier, a k-nearest neighbors classifier, and perhaps a few more*.\n",
    "- A very simple way to create an even better classifier is to aggregate the predictions of each classifier: the class that gets the most votes is the ensemble's prediction.\n",
    "- This majority-vote classifier is called a *hard-voting* classifier.\n",
    "- Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. \n",
    "- In fact, even if each classifier is a *weak learner* (meaning it only does slightly better than random guessing), the ensemble can still be a *strong learner* (achieving high accuracy), provided there are a sufficient number of weak learners in the ensemble and they are sufficiently diverse.\n",
    "\n",
    "***Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble's accuracy.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-Learn's `VotingClassifier` class:\n",
    "    - Just give it a list of name/predictor pairs, and use it like a normal classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using VotingClassifier on the make_moons dataset\n",
    "# We will load and split the moons dataset into a training set and a test set, then we'll create and train a voting classifier\n",
    "# composed of three diverse classifiers\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
